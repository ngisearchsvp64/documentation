% SPDX-License-Identifier: LGPL-3-or-later
% Copyright 2023 VectorCamp
% Copyright 2023 Red Semiconductor Ltd.
% Copyright 2023 VanTosh
%
% Funded by NGI Search Programme HORIZON-CL4-2021-HUMAN-01 2022,
% https://www.ngisearch.eu/, EU Programme 101069364.
\section{Exploring Several Important SIMD Operations}
\subsection{Vector Permute - \texttt{vperm}}

A very useful operation present in PowerISA as part of the
\acrshort{VSX}/\acrshort{VMX}.
A reference implementation of vector permute in Python looks like this:

\begin{verbatim}
def ref_vpermd(vec1, vec2, vec3):
    vecResult = [0]*16
    for i in range(16): # setvl VL=16
        if vec3[i] == 0:
            vecResult[i] = vec1[i]
        else:
            vecResult[i] = vec2[i]

    return vecResult
\end{verbatim}

\texttt{vperm} takes in three operand vectors, 16*8-bit (128-bit) each.
The third operand, \texttt{vec3} is used to determine whether an element from
\texttt{vec1} or \texttt{vec2} gets stored in the resulting vector.

\begin{itemize}
  \item For each \texttt{i}'th element in \texttt{vec3}, if element is
        equal to 0, store corresponding element from \texttt{vec1} in the result.
  \item Otherwise, store corresponding element from \texttt{vec2} in the result.
\end{itemize}

No re-arrangement happens (not a shuffle) and in hardware, this might be
implemented as 16 2-in 1-out byte multiplexers.

\subsubsection{SVP64 Draft Implementation}

An SVP64 implementation for this operation can be found
\href{https://github.com/ngisearchsvp64/glibc-svp64/blob/master/svp64-port/svp64/vperm_svp64.s}{here}.

The main assembler portion is this:
\begin{verbatim}
.main_loop:
  sv.cmpb             *maskOut, *vec3, mask0
  sv.and              *res, *vec1, *maskOut
  sv.cmpb             *maskOut, *vec3, mask1
  sv.and              *tmp, *vec2, *maskOut
  sv.or               *res, *res, *tmp
  svstep.             ctr, 1, 0
  # Branch if CTR!=0
  bdnz                .main_loop
\end{verbatim}

Half of \texttt{vec3} is compared with \texttt{mask0} (all zeros), and an output
mask is produced. This mask is then AND'd with half of \texttt{vec1}, and stored
in result.
Similar instructions occur for \texttt{vec2}, but a temporary is used, and then
OR'd with the result, which will provide the final \texttt{vperm} output.

\subsection{Shuffle}

Shuffle is a useful operation present in the Intel AVX extension, and it allows
the to move/permute bytes of vector \texttt{a} based on the bytes of vector
\texttt{b}.
Particular variant in question is 256-bit, 8-bit element (32x8) inputs.
The instruction is split into two 16-element lanes, for legacy reasons.

\begin{verbatim}
def ref__mm256_shuffle_epi8(a, b):
    r = [0] * 32
    for i in range(0, 16):
        # Lower half
        if b[i] & 0x80:
            r[i] = 0
        else:
            r[i] = a[b[i] & 0x0F]
        # Upper half
        if b[16+i] & 0x80:
            r[16+i] = 0
        else:
            r[16+i] = a[16+(b[16+i] & 0x0F)]

    return r
\end{verbatim}

With each lane the following happens:

\begin{itemize}
  \item For each \texttt{i}'th element in lane, check if \acrfull{MSb} of
        element in vector \texttt{b} is set. If set, clear the corresponding
        result element.
  \item Otherwise, use the lower nibble of element in \texttt{b} as an index
        for vector \texttt{a} and store in result.
\end{itemize}

The difference with the second lane is that an offset of 16 is added (since
each lane can \textit{only} access its own elements).

\subsubsection{\acrshort{SVP64} Draft Implementation}

The implementation shown below is not fully working due to time constraints.
Also, only the lower half the bytes in the shuffle is done.

\begin{verbatim}
  setvl 1, 0, 32, 0, 1, 0 # Horizontal-First mode
  # Create temp where only lower nibble of each element in b set.
  sv.andi/ew=8 *temp1_start, *vec_b_start, 0x0f
  mtspr 9, 16 # 32*8-bit elements, but only working on half
  # Check if upper bit set, then clear result element if so
  setvl 1, 0, 16, 0, 1, 1 # Vertical-First mode
.lower_loop:
  sv.andi/ew=8 *temp2_start, vec_b_start, 0x80
  sv.cmpi/ew=8 cr0, 1, *vec_b_start, 0x80
  #TODO: branch if b[i]==0x80
  sv.bc 12, 2, .clear",
  # Enable REMAP on operand RA,
  # ew=8 (0b11), mm=0, sk=0
  svindex "%d, 0b10000, 4, 3, 0, 0" % (vec_b_start>>2)
  sv.addi/ew=8 *vec_r_start, *vec_b_start, 0
  b .step
.clear:
  sv.addi *vec_r_start, 0, 0
.step:
  svstep. 0, 1, 0
  bdnz .lower_loop
  # upper half TODO
\end{verbatim}

The snippet above requires the Element Width Override functionality of
\acrshort{SVP64} in order to operate on individual bytes contained
in the vector to be shuffled.

\subsection{Movemask}

An instruction from Intel \acrshort{AVX}, which creates a bit mask from a
source vector.

\begin{verbatim}
# Operates on 32*8 (256-bit) value
def ref_movemask_epi8(s1):
    res = 0 # 32-bit value
    for i in range(32): # setvl VL=32
        t1 = s1[i] & 0x80
        if t1 == 0x80:
            t2 = 1<<i
            res |= t2

    return res
\end{verbatim}

\begin{itemize}
  \item For each \texttt{i}'th element in \texttt{s1}, if \acrshort{MSb} of
        element \texttt{i} is equal to 1, set bit \texttt{i} of the result.
\end{itemize}

\subsubsection{\acrshort{SVP64} Draft Implementation}

\begin{verbatim}
  setvl 1, 0, 4, 1, 1, 1",
.svstep_loop:
  addi shiftR_cnt, 0, 0 # clear shift amount
  mtspr 9, 8",
.inner_loop:
  # SVP64Asm() seems to incorrectly convert vector operand for srd,
  # Using temp reg
  sv.add *temp1_start, *vec_s1_start, zero
  sv.srd *temp2_start, *temp1_start, shiftR_cnt
  addi shiftR_cnt, shiftR_cnt, 8 # Increase shift value
  sv.andi. *%d, *%d, 0x80" % (temp2_start, temp2_start),
  sv.cmpi *cr0, 1, *%d, 0x80" % (temp2_start),
  sv.bc 4, *2, 0x14 # Not equal to 0x80
  # Set a result bit corresponding to input vector element's MSb
  sv.sld *temp2_start, const_1, shiftL_cnt
  sv.or *vecResult_start, *vecResult_start, *temp2_start
  bc 16, 0, -0x3c",
  svstep. 0, 1, 0",
  addi vl_count, vl_count, -1
  cmpi cr0, 1, vl_count, 0
  bc 4, 2, -0x50
\end{verbatim}
