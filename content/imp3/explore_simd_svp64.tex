% SPDX-License-Identifier: LGPL-3-or-later
% Copyright 2023 VectorCamp
% Copyright 2023 Red Semiconductor Ltd.
% Copyright 2023 VanTosh
%
% Funded by NGI Search Programme HORIZON-CL4-2021-HUMAN-01 2022,
% https://www.ngisearch.eu/, EU Programme 101069364.
\section{Exploring Several Important \acrshort{SIMD} Operations}
\subsection{Vector Permute - \texttt{vperm}}

A very useful operation present in PowerISA as part of the
\acrshort{VSX}/\acrshort{VMX}.
A reference implementation of vector permute in Python looks like this:

\begin{verbatim}
def ref_vpermd(vec1, vec2, vec3):
    vecResult = [0]*16
    for i in range(16): # setvl VL=16
        if vec3[i] == 0:
            vecResult[i] = vec1[i]
        else:
            vecResult[i] = vec2[i]

    return vecResult
\end{verbatim}

\texttt{vperm} takes in three operand vectors, 16*8-bit (128-bit) each.
The third operand, \texttt{vec3} is used to determine whether an element from
\texttt{vec1} or \texttt{vec2} gets stored in the resulting vector.

\begin{itemize}
  \item For each \texttt{i}'th element in \texttt{vec3}, if element is
        equal to 0, store corresponding element from \texttt{vec1} in the result.
  \item Otherwise, store corresponding element from \texttt{vec2} in the result.
\end{itemize}

No re-arrangement happens (not a shuffle) and in hardware, this might be
implemented as 16 2-in 1-out byte multiplexers.

\subsubsection{\acrshort{SVP64} Draft Implementation}

An \acrshort{SVP64} implementation for this operation can be found
\href{https://github.com/ngisearchsvp64/glibc-svp64/blob/master/svp64-port/svp64/vperm_svp64.s}{here}.

The main assembler portion is this:
\begin{verbatim}
.main_loop:
  sv.cmpb             *maskOut, *vec3, mask0
  sv.and              *res, *vec1, *maskOut
  sv.cmpb             *maskOut, *vec3, mask1
  sv.and              *tmp, *vec2, *maskOut
  sv.or               *res, *res, *tmp
  svstep.             ctr, 1, 0
  # Branch if CTR!=0
  bdnz                .main_loop
\end{verbatim}

Half of \texttt{vec3} is compared with \texttt{mask0} (all zeros), and an output
mask is produced. This mask is then AND'd with half of \texttt{vec1}, and stored
in result.
Similar instructions occur for \texttt{vec2}, but a temporary is used, and then
OR'd with the result, which will provide the final \texttt{vperm} output.

\subsection{Shuffle}

Shuffle is a useful operation present in the Intel \acrshort{AVX} extension,
and it allows the to move/permute bytes of vector \texttt{a} based on the
bytes of vector \texttt{b}.
Particular variant in question is 256-bit, 8-bit element (32x8) inputs.
The instruction is split into two 16-element lanes, for legacy reasons.

\begin{verbatim}
def ref__mm256_shuffle_epi8(a, b):
    r = [0] * 32
    for i in range(0, 16):
        # Lower half
        if b[i] & 0x80:
            r[i] = 0
        else:
            r[i] = a[b[i] & 0x0F]
        # Upper half
        if b[16+i] & 0x80:
            r[16+i] = 0
        else:
            r[16+i] = a[16+(b[16+i] & 0x0F)]

    return r
\end{verbatim}

With each lane the following happens:

\begin{itemize}
  \item For each \texttt{i}'th element in lane, check if \acrfull{MSb} of
        element in vector \texttt{b} is set. If set, clear the corresponding
        result element.
  \item Otherwise, use the lower nibble of element in \texttt{b} as an index
        for vector \texttt{a} and store in result.
\end{itemize}

The difference with the second lane is that an offset of 16 is added (since
each lane can \textit{only} access its own elements).

\subsubsection{\acrshort{SVP64} Draft Implementation}

The implementation shown below is not fully working due to time constraints
and issues with running on the simulator.
Also, only the lower half the bytes in the shuffle is done.

This snippet requires the Element Width Override functionality of
\acrshort{SVP64} in order to operate on individual bytes contained
in the vector to be shuffled.

\begin{verbatim}
  setvl 1, 0, 32, 0, 1, 0 # Horizontal-First mode
  # Create temp where only lower nibble of each element in b set.
  sv.andi/ew=8 *temp1_start, *vec_b_start, 0x0f
  mtspr 9, 16 # 32*8-bit elements, but only working on half
  # Check if upper bit set, then clear result element if so
  setvl 1, 0, 16, 0, 1, 1 # Vertical-First mode
.lower_loop:
  sv.andi/ew=8 *temp2_start, vec_b_start, 0x80
  sv.cmpi/ew=8 cr0, 1, *vec_b_start, 0x80
  #TODO: branch if b[i]==0x80
  sv.bc 12, 2, .clear",
  # Enable REMAP on operand RA,
  # ew=8 (0b11), mm=0, sk=0
  svindex "%d, 0b10000, 4, 3, 0, 0" % (vec_b_start>>2)
  sv.addi/ew=8 *vec_r_start, *vec_a_start, 0
  b .step
.clear:
  sv.addi *vec_r_start, 0, 0
.step:
  svstep. 0, 1, 0
  bdnz .lower_loop
  # upper half TODO
\end{verbatim}

\subsubsection{Breakdown}

\begin{verbatim}
  setvl 1, 0, 32, 0, 1, 0 # Horizontal-First mode
  # Create temp where only lower nibble of each element in b set.
  sv.andi/ew=8 *temp1_start, *vec_b_start, 0x0f
\end{verbatim}

This code works in Horizontal-First mode to create a copy of vector \texttt{b}
where the upper nibble of each byte is masked out (because when an element
from \texttt{b} used to index \texttt{a}, only elements 0-15 are permitted).

\begin{verbatim}
.lower_loop:
  sv.andi/ew=8 *temp2_start, vec_b_start, 0x80
  sv.cmpi/ew=8 cr0, 1, *vec_b_start, 0x80
  #TODO: branch if b[i]==0x80
  sv.bc 12, 2, .clear",
\end{verbatim}

Here each byte/element's \acrshort{MSb} of vector \texttt{b} is tested.
If the \acrshort{MSb} is set, then the corresponding byte of the result vector
is cleared. Otherwise the lower nibble of element \texttt{b} is used as an
index for vector \texttt{a} (done in the next part of code).

\begin{verbatim}
  # Enable REMAP on operand RA,
  # ew=8 (0b11), mm=0, sk=0
  svindex "%d, 0b10000, 1, 3, 0, 0" % (vec_b_start>>2)
  sv.addi/ew=8 *vec_r_start, *vec_a_start, 0
  b .step
\end{verbatim}

The \texttt{svindex} instruction is used to enable Indexed REMAP mode.
The REMAP system is used to access vector elements according to predefined
schedule (other then the usual \texttt{0...VL-1}). The Indexed mode allows
to use elements inside a \acrshort{GPR} (in this case vector \texttt{b})
as an index to vector \texttt{a} (hence performing a shuffle operation).

The argument settings for the \texttt{svindex} instruction are as follows:

\begin{itemize}
  \item \texttt{SVG} - The register to use for indexing. The actual
  \acrshort{GPR} number is shifted by 2 (divided by 4), so if vector \texttt{b}
  was to start at \acrshort{GPR} \#16, then the \texttt{SVG} would be
  \texttt{SVG = 16 >> 2 = 4 (or 0b0100)}.
  \item \texttt{rmm} - REMAP mask. The REMAP can have four schedules, and they
  can be assigned to the 3 operands (\texttt{RA}, \texttt{RB}, \texttt{RC})
  and 2 results (\texttt{RT}, \texttt{RS}). This is a 5-bit parameter which
  specifies which operands and results will use the REMAP system. In this case,
  only the input \texttt{RA} needs to use the Indexed REMAP mode, as the output
  is iterated over sequentially (no need for REMAP).
  \item \texttt{SVd} - \acrshort{SV} REMAP x/y dimension setting. Normally,
  REMAP schedules have three dimensions, but with the indexed mode, the third
  dimension is disabled. For straight indexing, this is set to 1.
  \item \texttt{ew} - Element Width Override setting. \texttt{3 (0b11)}
  corresponds to 8-bit element values
  (as the code is processing individual bytes).
  \item \texttt{SVyx} - Reordering setting, not used here.
  \item \texttt{mm} - Mask mode. 0 is default mode.
  \item \texttt{sk} - Skip dimension. Not needed here.
\end{itemize}

The following prefixed \texttt{addi} instruction will then access the byte
elements from vector \texttt{a} using the indices stored in vector \texttt{b}.

\begin{verbatim}
.clear:
  sv.addi/ew=8 *vec_r_start, 0, 0
\end{verbatim}

The \texttt{.clear} section is only accessed if the result vector byte is to be cleared

\begin{verbatim}
.step:
  svstep. 0, 1, 0
  bdnz .lower_loop
\end{verbatim}

Here the current step of the \acrshort{SVP64} system is incremented using the
\texttt{svstep} instruction. When all step values have been iterated through
(0-15), then the conditional branch will fail, and the first lane of the result
vector will be correctly shuffled.

For now the upper half hasn't been written, however it would look pretty much
identical, with the only difference being an offset of 16 in the \texttt{a}
and \texttt{b} vectors. The simple way to do this is simply to define where
the halfway register for \texttt{a} and \texttt{b} are, and duplicate
the entire code above (but instead of \texttt{vec\_a\_start} use
\texttt{vec\_a\_half}, etc.).
However, REMAP's offset parameter could be used to write
both lane shuffling parts in a shorter code snippet (the extra uses of the
REMAP system would incur a hardware penalty (versus simply starting at a
halfway register and computing without the REMAP system) however.

\subsection{Movemask}

An instruction from Intel \acrshort{AVX}, which creates a bit mask from a
source vector.

\begin{verbatim}
# Operates on 32*8 (256-bit) value
def ref_movemask_epi8(s1):
    res = 0 # 32-bit value
    for i in range(32): # setvl VL=32
        t1 = s1[i] & 0x80
        if t1 == 0x80:
            t2 = 1<<i
            res |= t2

    return res
\end{verbatim}

\begin{itemize}
  \item For each \texttt{i}'th element in \texttt{s1}, if \acrshort{MSb} of
        element \texttt{i} is equal to 1, set bit \texttt{i} of the result.
\end{itemize}

\subsubsection{\acrshort{SVP64} Draft Implementation}

\begin{verbatim}
  setvl 1, 0, 4, 1, 1, 1",
.svstep_loop:
  addi shiftR_cnt, 0, 0 # clear shift amount
  mtspr 9, 8",
.inner_loop:
  # SVP64Asm() seems to incorrectly convert vector operand for srd,
  # Using temp reg
  sv.add *temp1_start, *vec_s1_start, zero
  sv.srd *temp2_start, *temp1_start, shiftR_cnt
  addi shiftR_cnt, shiftR_cnt, 8 # Increase shift value
  sv.andi. *%d, *%d, 0x80" % (temp2_start, temp2_start),
  sv.cmpi *cr0, 1, *%d, 0x80" % (temp2_start),
  sv.bc 4, *2, 0x14 # Not equal to 0x80
  # Set a result bit corresponding to input vector element's MSb
  sv.sld *temp2_start, const_1, shiftL_cnt
  sv.or *vecResult_start, *vecResult_start, *temp2_start
  bc 16, 0, -0x3c",
  svstep. 0, 1, 0",
  addi vl_count, vl_count, -1
  cmpi cr0, 1, vl_count, 0
  bc 4, 2, -0x50
\end{verbatim}
